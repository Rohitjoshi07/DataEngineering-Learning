{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66eaa9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8922fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/rohit/spark/spark-3.2.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/09 18:14:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"app\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d1a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0dc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option('header','true') \\\n",
    "        .schema(schema) \\\n",
    "        .csv('fhvhv_tripdata_2021-06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85a1f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|              B02764|2021-06-01 00:02:41|2021-06-01 00:07:46|         174|          18|      N|\n",
      "|              B02764|2021-06-01 00:16:16|2021-06-01 00:21:14|          32|         254|      N|\n",
      "|              B02764|2021-06-01 00:27:01|2021-06-01 00:42:11|         240|         127|      N|\n",
      "|              B02764|2021-06-01 00:46:08|2021-06-01 00:53:45|         127|         235|      N|\n",
      "|              B02510|2021-06-01 00:45:42|2021-06-01 01:03:33|         144|         146|      N|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/09 18:16:58 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 6\n",
      "CSV file: file:///home/rohit/data-engineering-zoomcamp/week_5_batch_processing/homework/fhvhv_tripdata_2021-06.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffc890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff21b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/05 17:57:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag, Affiliated_base_number\n",
      " Schema: hvfhs_license_num, dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag\n",
      "Expected: hvfhs_license_num but found: dispatching_base_num\n",
      "CSV file: file:///home/rohit/data-engineering-zoomcamp/week_5_batch_processing/homework/fhvhv_tripdata_2021-06.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"fhvh/2021/06\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91afa3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e5a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/spark/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"trip_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0742901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ANSWER1:  how many taxi trips were started on june 15\n",
    "res1 = spark.sql(\n",
    "\"\"\"\n",
    "SELECT count(*)\n",
    "FROM trip_data\n",
    "where DATE(pickup_datetime)='2021-06-15'\n",
    "\"\"\")\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80269bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    " 'dispatching_base_num',\n",
    " 'pickup_date',\n",
    " 'dropoff_date',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'SR_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938c2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)).select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110034db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/09 18:24:25 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 6\n",
      "CSV file: file:///home/rohit/data-engineering-zoomcamp/week_5_batch_processing/homework/fhvhv_tripdata_2021-06.csv\n",
      "[Stage 8:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+------------+------------+-------+\n",
      "|dispatching_base_num|pickup_date|dropoff_date|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-----------+------------+------------+------------+-------+\n",
      "|              B02875| 2021-06-01|  2021-06-01|         138|         144|      N|\n",
      "|              B02884| 2021-06-04|  2021-06-04|          83|         265|      N|\n",
      "+--------------------+-----------+------------+------------+------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47ba7578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|trip_time|\n",
      "+---------+\n",
      "|       66|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer 2: longest trip duration\n",
    "spark.sql('''\n",
    "\n",
    "select  EXTRACT(days from  (dropoff_datetime - pickup_datetime))*24 + EXTRACT(hours from  (dropoff_datetime - pickup_datetime)) as trip_time\n",
    "from trip_data order by trip_time desc LIMIT 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "854eb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.option('header','true').csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc46f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------+\n",
      "|LocationID|Borough|                Zone|service_zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "|         1|    EWR|      Newark Airport|         EWR|\n",
      "|         2| Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|  Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f053f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER 4: name of most frequent pickup location zones\n",
    "\n",
    "combined_df = df.join(df_zones,  df.PULocationID == df_zones.LocationID)\n",
    "combined_df = combined_df.drop('LocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "668a13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/09 19:16:36 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 6\n",
      "CSV file: file:///home/rohit/data-engineering-zoomcamp/week_5_batch_processing/homework/fhvhv_tripdata_2021-06.csv\n",
      "[Stage 34:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+------------+------------+-------+----------+-------+-----------------+------------+\n",
      "|dispatching_base_num|pickup_date|dropoff_date|PULocationID|DOLocationID|SR_Flag|LocationID|Borough|             Zone|service_zone|\n",
      "+--------------------+-----------+------------+------------+------------+-------+----------+-------+-----------------+------------+\n",
      "|              B02875| 2021-06-01|  2021-06-01|         138|         144|      N|       138| Queens|LaGuardia Airport|    Airports|\n",
      "|              B02884| 2021-06-04|  2021-06-04|          83|         265|      N|        83| Queens| Elmhurst/Maspeth|   Boro Zone|\n",
      "|              B02875| 2021-06-04|  2021-06-04|         129|         255|      N|       129| Queens|  Jackson Heights|   Boro Zone|\n",
      "+--------------------+-----------+------------+------------+------------+-------+----------+-------+-----------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23926abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/spark/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "combined_df.registerTempTable(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93e9aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------+\n",
      "|PULocationID|               Zone|pickup_count|\n",
      "+------------+-------------------+------------+\n",
      "|          61|Crown Heights North|      231279|\n",
      "+------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ANSWER 4: name of most frequent pickup location zones\n",
    "val = spark.sql('''\n",
    "\n",
    "select PULocationID, Zone, count(*) as pickup_count\n",
    "from combined_data \n",
    "group by PULocationID, Zone HAVING\n",
    "pickup_count = (select MAX(pickup_count) FROM\n",
    "(select PULocationID, count(*) as pickup_count from combined_data \n",
    "GROUP BY PULocationID) AS pickup_counts)\n",
    "limit 1\n",
    "''')\n",
    "val.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51af0535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_date',\n",
       " 'dropoff_date',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Borough',\n",
       " 'Zone',\n",
       " 'service_zone']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4b04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
